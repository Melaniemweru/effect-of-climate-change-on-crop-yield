{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "56daf526"
   },
   "source": [
    "### Introduction\n",
    "Climate change presents a profound challenge to agricultural productivity worldwide. The frequency of extreme weather events, variability in rainfall patterns, and shifts in temperature are all contributing to increasing uncertainty in agricultural outputs. In regions heavily reliant on agriculture, such as many parts of Kenya, the impact of these changes can have severe implications not just for local economies, but also for food security and overall societal stability.\n",
    "\n",
    "### Project Overview\n",
    "This project aims to leverage extensive meteorological and agricultural data to analyze the impacts of climate change on crop yields. By examining historical data and identifying patterns and trends, the project seeks to forecast future conditions and provide actionable insights that can help mitigate the adverse effects of climate change on agriculture.\n",
    "\n",
    "#### Business Problem\n",
    "The primary challenge addressed by this project is the need for enhanced predictive capabilities that can accurately forecast the impacts of various climatic factors on crop yields. Current agricultural planning and operations often rely on traditional knowledge and simplistic predictive models that fail to account for the increasing complexity and variability introduced by climate change.\n",
    "\n",
    "#### Stakeholders\n",
    "Farmers and Agricultural Cooperatives: Direct beneficiaries who require accurate predictions to make informed decisions about crop planning and resource allocation.\n",
    "Government Agencies: Need insights to formulate effective agricultural policies and climate adaptation strategies.\n",
    "Agricultural Researchers and Academics: Interested in data-driven insights to enhance scientific understanding and develop more resilient agricultural practices.\n",
    "NGOs and International Aid Organizations: Require accurate forecasts to better target their support and interventions in regions most affected by climate change.\n",
    "#### Objectives\n",
    "Analyze Historical Data: Understand how temperature, rainfall, and other factors have historically affected crop yields.\n",
    "Develop Predictive Models: Create robust models that can predict crop yields based on various climatic inputs.\n",
    "Provide Decision Support: Offer actionable recommendations that can help stakeholders adjust to changing conditions more effectively.\n",
    "Enhance Climate Resilience: Contribute to building a more climate-resilient agricultural sector by integrating advanced analytics into farming practices.\n",
    "##### Research Questions\n",
    "How do variations in temperature and rainfall correlate with changes in crop yields?\n",
    "Which crops are most vulnerable to changes in specific climatic factors?\n",
    "Can we predict crop yield changes based on forecasted weather patterns?\n",
    "What are the most effective strategies for mitigating adverse climate impacts on agriculture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "badd4d88",
    "outputId": "6dee4b7d-61bc-47f3-b66d-8317dc75bf88"
   },
   "outputs": [],
   "source": [
    "!pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "id": "6c0fc9fc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "id": "18cd74d3"
   },
   "outputs": [],
   "source": [
    "yield_csv = \"/content/climate-ds.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "id": "604f7caa"
   },
   "outputs": [],
   "source": [
    "yield_df = pd.read_csv(yield_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "e7c5769a",
    "outputId": "c63e780a-cdaa-4f18-f782-2b5a2ee98219"
   },
   "outputs": [],
   "source": [
    "yield_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0f331cd6",
    "outputId": "fc11e33c-d363-44a3-f1ae-caeb9a0f25bd"
   },
   "outputs": [],
   "source": [
    "yield_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f19d60c1",
    "outputId": "93de278c-ea2b-46c0-8455-9a28233393d7"
   },
   "outputs": [],
   "source": [
    "# Dropping the 'Unnamed: 0' column from the DataFrame\n",
    "yield_df = yield_df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# Verifying that the column has been dropped\n",
    "print(yield_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cd1b99c4",
    "outputId": "b7e9f282-5db1-4ada-f45f-55d82e818027"
   },
   "outputs": [],
   "source": [
    "yield_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "d9ed4cf2",
    "outputId": "836d99c0-c51a-4890-bf59-733419063be7"
   },
   "outputs": [],
   "source": [
    "yield_df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "32ebf394",
    "outputId": "fa958b23-86ad-4c60-8ade-7ad270083dd3"
   },
   "outputs": [],
   "source": [
    "yield_df['Area'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "id": "ddfc9936"
   },
   "outputs": [],
   "source": [
    "# india is the most frequent ,followed by Brazil and Mexico\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429
    },
    "id": "1ad6d058",
    "outputId": "45e0ad32-992b-4b3d-bbfc-97a1ee0c26cf"
   },
   "outputs": [],
   "source": [
    "yield_df['Item'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "id": "737b01a1"
   },
   "outputs": [],
   "source": [
    "# Potatoes are the most popular crop followed by Maize and Wheat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "id": "544803a2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "id": "4a052be0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "id": "358039ee"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "id": "29f0ccb7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "id": "ff084af9",
    "outputId": "de2b6aa6-9c95-4495-ee9f-79df612ebb99"
   },
   "outputs": [],
   "source": [
    "yield_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b5f0f3b7",
    "outputId": "051c8bee-49b6-473f-99ec-a9093249fd87"
   },
   "outputs": [],
   "source": [
    "yield_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aa8f49ab",
    "outputId": "a247bafb-57ad-4e51-9cce-dd4c3f3fb8de"
   },
   "outputs": [],
   "source": [
    "# Display some duplicate rows to understand their nature\n",
    "print(yield_df[yield_df.duplicated(keep=False)].sort_values(by=['Area', 'Year', 'Item']).head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "381d3da4",
    "outputId": "1be53de2-ecd8-4dc6-ff8a-50de6d534985"
   },
   "outputs": [],
   "source": [
    "# Remove duplicates considering specific columns that define a unique entry\n",
    "yield_df = yield_df.drop_duplicates(subset=['Area', 'Item', 'Year', 'hg/ha_yield', 'average_rain_fall_mm_per_year', 'pesticides_tonnes', 'avg_temp'])\n",
    "\n",
    "# Verify the removal by checking for duplicates again\n",
    "print(\"Remaining duplicates:\", yield_df.duplicated(subset=['Area', 'Item', 'Year', 'hg/ha_yield', 'average_rain_fall_mm_per_year', 'pesticides_tonnes', 'avg_temp']).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6f53f62f",
    "outputId": "1b32ad96-5c4c-4d3c-f477-5c92e3baf8d7"
   },
   "outputs": [],
   "source": [
    "# Check the new shape of the DataFrame\n",
    "print(\"Updated DataFrame shape:\", yield_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "119fa23a",
    "outputId": "1efa697d-129d-4dbd-f8dc-3030de16de1a"
   },
   "outputs": [],
   "source": [
    "yield_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "139a52ea",
    "outputId": "a14d3c99-fde8-46b8-e08a-d962630b4fe1"
   },
   "outputs": [],
   "source": [
    "yield_df['pesticides_tonnes'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "380ef8a7",
    "outputId": "5f19d62d-4c17-417a-b7e6-c5b90e5543c4"
   },
   "outputs": [],
   "source": [
    "yield_df['average_rain_fall_mm_per_year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "id": "f568e8c9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "843f34e0",
    "outputId": "51fe1abc-8b0e-4d1c-c0b2-7f15fbeb1510"
   },
   "outputs": [],
   "source": [
    "# Group by 'Area', sum the yields, and find the top 10 countries with the highest total yields\n",
    "top_countries_by_yield = yield_df.groupby('Area', sort=True)['hg/ha_yield'].sum().nlargest(10)\n",
    "\n",
    "# Display the result\n",
    "print(top_countries_by_yield)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 546
    },
    "id": "f6443d57",
    "outputId": "b682e126-55a4-46ca-dcc8-eb68007a1112"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plotting the result\n",
    "top_countries_by_yield.plot(kind='bar', color='skyblue')\n",
    "plt.title('Top 10 Countries by Total Crop Yield')\n",
    "plt.xlabel('Country')\n",
    "plt.ylabel('Total Crop Yield (hg/ha)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {
    "id": "e24a0e00"
   },
   "source": [
    " india is the highest crop yeilds followed by Brazil and mexico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "id": "ec0d6fd0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4d24ebba",
    "outputId": "d2a18833-f478-4441-aa3d-1d1b874e147b"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Histograms for all numeric data\n",
    "yield_df.hist(bins=15, figsize=(15, 10))\n",
    "plt.suptitle('Histograms of All Variables')\n",
    "plt.show()\n",
    "\n",
    "# Box plots to identify outliers\n",
    "for column in ['hg/ha_yield', 'average_rain_fall_mm_per_year', 'pesticides_tonnes', 'avg_temp']:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.boxplot(x=yield_df[column])\n",
    "    plt.title(f'Box Plot of {column}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {
    "id": "6bd3b6b3"
   },
   "source": [
    "The average yeild is between 0 and 100000 hg/ha ,the average rainfall is between 500 and 1500,the average pesticide used is between 0-50000tons and the average temperature is between 25 and 28C\n",
    "\n",
    "there is also presence of outlier in the data and therefore some transformations may be needed before modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 495
    },
    "id": "b279b8ee",
    "outputId": "757467d7-8920-46bb-a4b1-22042975d05f"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Adding 1 to ensure there are no log(0) issues\n",
    "yield_df['log_hg/ha_yield'] = np.log1p(yield_df['hg/ha_yield'])\n",
    "\n",
    "\n",
    "# Check for any zero or negative values in rainfall if you want to include it\n",
    "# yield_df['log_average_rain_fall_mm_per_year'] = np.log1p(yield_df['average_rain_fall_mm_per_year'])\n",
    "\n",
    "# Updated list of variables to plot\n",
    "variables_transformed = ['log_hg/ha_yield', 'pesticides_tonnes', 'avg_temp','average_rain_fall_mm_per_year']\n",
    "\n",
    "# Replotting histograms to check improvements\n",
    "yield_df[variables_transformed].hist(bins=15, figsize=(15, 5), layout=(1, 4))\n",
    "plt.suptitle('Histograms of Log-Transformed Variables')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "id": "5c654a7f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 843
    },
    "id": "3bd41b97",
    "outputId": "6dbb1389-1996-4abb-ac68-498ce03cc7f3"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# List of transformed variables to plot\n",
    "variables_transformed = ['log_hg/ha_yield', 'pesticides_tonnes', 'avg_temp']\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(18, 12))\n",
    "\n",
    "# Number of rows needed for subplots\n",
    "num_vars = len(variables_transformed)\n",
    "rows = num_vars\n",
    "\n",
    "# Loop through the list of variables and create a subplot for each histogram and box plot\n",
    "for i, var in enumerate(variables_transformed, 1):\n",
    "    plt.subplot(rows, 2, 2 * i - 1)  # Odd index for histograms\n",
    "    sns.histplot(yield_df[var], kde=True)\n",
    "    plt.title(f'Histogram of {var}')\n",
    "\n",
    "    plt.subplot(rows, 2, 2 * i)  # Even index for box plots\n",
    "    sns.boxplot(x=yield_df[var])\n",
    "    plt.title(f'Box Plot of {var}')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "1a4206d6",
    "outputId": "6fb33c5e-cbec-441a-c138-da0b458b0448"
   },
   "outputs": [],
   "source": [
    "# Define a function to remove outliers based on IQR\n",
    "def remove_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "\n",
    "# Apply to the log-transformed yield\n",
    "cleaned_df = remove_outliers(yield_df, 'log_hg/ha_yield')\n",
    "\n",
    "# Plotting to verify removal of outliers\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(cleaned_df['log_hg/ha_yield'], kde=True)\n",
    "plt.title('Histogram After Removing Outliers')\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(x=cleaned_df['log_hg/ha_yield'])\n",
    "plt.title('Box Plot After Removing Outliers')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {
    "id": "69a833ab"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {
    "id": "6d8bf247"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {
    "id": "fe64b5dd"
   },
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "corr_matrix = cleaned_df[['hg/ha_yield', 'average_rain_fall_mm_per_year', 'pesticides_tonnes', 'avg_temp','log_hg/ha_yield']].corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 851
    },
    "id": "2c4a6780",
    "outputId": "7b0d8161-12d3-40dc-ce83-6f2112f0acd3"
   },
   "outputs": [],
   "source": [
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', square=True, linewidths=.5, cbar_kws={\"shrink\": .8})\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title('Correlation Matrix of Crop Data')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {
    "id": "1dedb52e"
   },
   "source": [
    "The correlation between the log-transformed crop yield (log_hg/ha_yield) and other variables like rainfall, temperature, and pesticide usage appears relatively low.\n",
    "The strongest correlations involving log_hg/ha_yield are with the original yield values, which is expected due to their direct mathematical relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "539d394a",
    "outputId": "a984cdb2-1e91-4ea1-8287-271662b3c54e"
   },
   "outputs": [],
   "source": [
    "# Pairplot to explore the relationships between all continuous variables\n",
    "sns.pairplot(cleaned_df[['log_hg/ha_yield', 'average_rain_fall_mm_per_year', 'pesticides_tonnes', 'avg_temp']])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {
    "id": "4686cba3"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "bb90c1b3",
    "outputId": "d31b585d-46d2-4d73-fd04-7ebb0c508032"
   },
   "outputs": [],
   "source": [
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "47b12e8a",
    "outputId": "17af5195-c7cc-4ac9-ef14-8522da1ae3b1"
   },
   "outputs": [],
   "source": [
    "cleaned_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9fd60170",
    "outputId": "deee8880-5219-448d-d416-631b97c3708f"
   },
   "outputs": [],
   "source": [
    "cleaned_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "bde19e8d",
    "outputId": "42936b24-1f33-475e-c40e-6d3a2b0a50e5"
   },
   "outputs": [],
   "source": [
    "\n",
    "X = cleaned_df.drop(columns=[\"hg/ha_yield\", \"log_hg/ha_yield\"])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "605b4c0c",
    "outputId": "5ac1bf80-055d-4aba-e963-7b5bff59e657"
   },
   "outputs": [],
   "source": [
    "print(X.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4929cd15",
    "outputId": "cc0b348b-393f-4196-fd0d-f9a08974f0b3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'X' is your features DataFrame and 'cleaned_df' contains the target variable 'log_hg/ha_yield'\n",
    "\n",
    "# Updating the list of numeric and categorical features\n",
    "numeric_features = ['avg_temp', 'average_rain_fall_mm_per_year', 'pesticides_tonnes']\n",
    "categorical_features = ['Area', 'Item']  # Added Item as a categorical feature\n",
    "\n",
    "# Setting up the preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Applying the preprocessing to the DataFrame\n",
    "X_preprocessed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Print categories used during training\n",
    "print(preprocessor.named_transformers_['cat'].categories_)\n",
    "\n",
    "# Target variable remains the same\n",
    "y = cleaned_df['log_hg/ha_yield']\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "numeric_feature_names = numeric_features\n",
    "categorical_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)\n",
    "\n",
    "# Combine all feature names\n",
    "feature_names = np.concatenate([numeric_feature_names, categorical_feature_names])\n",
    "\n",
    "# Ensure the lengths match for verification\n",
    "print(f\"Length of feature_names: {len(feature_names)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {
    "id": "f049cfe0"
   },
   "source": [
    "### baseline Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "796a5ef4",
    "outputId": "65682edc-3894-4201-970c-07080253931d"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Linear Regression Model\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "y_pred_linear = linear_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_linear = mean_squared_error(y_test, y_pred_linear)\n",
    "r2_linear = r2_score(y_test, y_pred_linear)\n",
    "print(f\"Linear Regression MSE: {mse_linear}\")\n",
    "print(f\"Linear Regression R^2: {r2_linear}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {
    "id": "f2569180"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "49bb16cc",
    "outputId": "36a8947a-c8fc-4ce7-afb5-1ed96a4ada6d"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = y_test - y_pred_linear\n",
    "\n",
    "# Plotting residuals\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Residuals Scatter Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_pred_linear, residuals, alpha=0.5)\n",
    "plt.title('Residuals vs. Predicted Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "\n",
    "# Histogram of Residuals\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(residuals, kde=True)\n",
    "plt.title('Distribution of Residuals')\n",
    "plt.xlabel('Residuals')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {
    "id": "lYfAUZfhOcFn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "f9022f72",
    "outputId": "784eb617-e23d-458f-a698-a96c60b401e4"
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "sm.qqplot(residuals, line='s')\n",
    "plt.title('Q-Q Plot of Residuals')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "272c08fc",
    "outputId": "661fd1d5-2cba-49b2-c6c3-f759a3b01b18"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_pred_linear, y_test, alpha=0.5)\n",
    "plt.title('Observed vs. Predicted Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Observed Values')\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--')  # Diagonal line for reference\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "72d96f58",
    "outputId": "4be02cc7-310a-42d3-ceb2-ef2183e39fc6"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(linear_model, X_preprocessed, y, cv=5, scoring='r2')\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-validated R² scores:\", cv_scores)\n",
    "print(\"Mean CV R²:\", np.mean(cv_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {
    "id": "7fa8706f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1bece06c",
    "outputId": "3696f3d1-7be7-4ef7-d7ee-3e7b59de1e95"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Adding polynomial features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly.fit_transform(X_train)\n",
    "\n",
    "# Refit the linear model with polynomial features\n",
    "linear_model_poly = LinearRegression()\n",
    "linear_model_poly.fit(X_poly, y_train)\n",
    "y_pred_poly = linear_model_poly.predict(poly.transform(X_test))\n",
    "\n",
    "# Evaluate the new model\n",
    "mse_poly = mean_squared_error(y_test, y_pred_poly)\n",
    "r2_poly = r2_score(y_test, y_pred_poly)\n",
    "print(f\"Polynomial Regression MSE: {mse_poly}\")\n",
    "print(f\"Polynomial Regression R²: {r2_poly}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {
    "id": "542891ca"
   },
   "source": [
    "### regulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6418cf89",
    "outputId": "f6cd4dfe-8853-44df-a35a-f43d8b171139"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "\n",
    "# Ridge Regression\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "y_pred_ridge = ridge_model.predict(X_test)\n",
    "\n",
    "# Lasso Regression\n",
    "lasso_model = Lasso(alpha=0.1)\n",
    "lasso_model.fit(X_train, y_train)\n",
    "y_pred_lasso = lasso_model.predict(X_test)\n",
    "\n",
    "# Elastic Net Regression\n",
    "elastic_model = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "elastic_model.fit(X_train, y_train)\n",
    "y_pred_elastic = elastic_model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Ridge R²:\", r2_score(y_test, y_pred_ridge))\n",
    "print(\"Lasso R²:\", r2_score(y_test, y_pred_lasso))\n",
    "print(\"Elastic Net R²:\", r2_score(y_test, y_pred_elastic))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {
    "id": "d573f121"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a56957e5",
    "outputId": "fd99fd71-07c8-4a0f-b7d0-e65c1379dd28"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "ridge_cv_scores = cross_val_score(ridge_model, X_preprocessed, y, cv=5, scoring='r2')\n",
    "print(\"Ridge CV R² scores:\", ridge_cv_scores)\n",
    "print(\"Ridge Mean CV R²:\", np.mean(ridge_cv_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {
    "id": "e0022794"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {
    "id": "00fda6a9"
   },
   "source": [
    "### Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 602
    },
    "id": "20ece2f1",
    "outputId": "25740200-bb37-4b5d-8b11-5959eeac6305"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "\n",
    "# Assuming that the preprocessing steps are done and X_train, X_test, y_train, y_test are defined\n",
    "# Assuming preprocessor is the ColumnTransformer used for preprocessing\n",
    "\n",
    "# Initialize the DecisionTreeRegressor\n",
    "tree_regressor = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "tree_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_tree = tree_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_tree = mean_squared_error(y_test, y_pred_tree)\n",
    "r2_tree = r2_score(y_test, y_pred_tree)\n",
    "print(f\"Decision Tree MSE: {mse_tree}\")\n",
    "print(f\"Decision Tree R²: {r2_tree}\")\n",
    "\n",
    "# Extract feature names after preprocessing\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "# Visualize the tree\n",
    "plt.figure(figsize=(20, 10))\n",
    "tree.plot_tree(tree_regressor, filled=True, feature_names=feature_names, rounded=True, fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {
    "id": "40121ad9"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "58c59689",
    "outputId": "0dff4d26-da4e-4446-f9fc-13d4dfc543f9"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Define the model\n",
    "tree_regressor = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Create a parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 10, 20],\n",
    "    'min_samples_leaf': [1, 5, 10],\n",
    "    'max_features': ['auto', 'sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=tree_regressor, param_grid=param_grid, cv=5, scoring='r2', verbose=1, n_jobs=-1)\n",
    "\n",
    "# Fit to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best cross-validated R² score: \", grid_search.best_score_)\n",
    "\n",
    "# Use the best estimator to make predictions\n",
    "best_tree = grid_search.best_estimator_\n",
    "y_pred_best_tree = best_tree.predict(X_test)\n",
    "\n",
    "# Evaluate the best model\n",
    "mse_best_tree = mean_squared_error(y_test, y_pred_best_tree)\n",
    "r2_best_tree = r2_score(y_test, y_pred_best_tree)\n",
    "print(f\"Optimized Decision Tree MSE: {mse_best_tree}\")\n",
    "print(f\"Optimized Decision Tree R²: {r2_best_tree}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {
    "id": "970a4800"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {
    "id": "1f31c9d0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {
    "id": "2f8656c2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {
    "id": "24fb7f84"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {
    "id": "7aa7b6d8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {
    "id": "4d7198cc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {
    "id": "a38e9ef6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {
    "id": "41a3c411"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {
    "id": "7cb1dd80"
   },
   "source": [
    "### Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "67131aa7",
    "outputId": "e890a275-7eb3-4f98-dab4-7214e127cf34"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Initialize the RandomForestRegressor\n",
    "random_forest = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Fit the model on the training data\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_rf = random_forest.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "print(f\"Random Forest MSE: {mse_rf}\")\n",
    "print(f\"Random Forest R²: {r2_rf}\")\n",
    "\n",
    "# Feature Importance\n",
    "feature_importances = random_forest.feature_importances_\n",
    "# Assuming we have feature names in a list called 'feature_names'\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances}).sort_values(by='Importance', ascending=False)\n",
    "print(importance_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {
    "id": "26cdb8dc"
   },
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "88ee6038",
    "outputId": "6f5e54ee-cf85-4415-da41-9c490dc46a31"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Set up parameter grid to tune the hyperparameters\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],  # Number of trees\n",
    "    'max_depth': [None, 10, 20],  # Maximum depth of trees\n",
    "    'min_samples_leaf': [1, 2, 4]  # Minimum samples at a leaf node\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=random_forest, param_grid=param_grid, cv=3, scoring='r2', n_jobs=-1)\n",
    "\n",
    "# Fit to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best R²:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {
    "id": "acf1fdce"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {
    "id": "e990a9b5"
   },
   "source": [
    "### Gradient Boosting Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fb502a4c",
    "outputId": "65586cab-34ba-4da7-8d7e-df3bdd717f70"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Train the Gradient Boosting model\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_gb = gb_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_gb = mean_squared_error(y_test, y_pred_gb)\n",
    "r2_gb = r2_score(y_test, y_pred_gb)\n",
    "print(f\"Gradient Boosting MSE: {mse_gb}\")\n",
    "print(f\"Gradient Boosting R²: {r2_gb}\")\n",
    "\n",
    "# Ensure the lengths match\n",
    "print(f\"Length of feature_importances_gb: {len(gb_regressor.feature_importances_)}\")\n",
    "\n",
    "# If lengths match, create DataFrame\n",
    "if len(feature_names) == len(gb_regressor.feature_importances_):\n",
    "    # Create DataFrame\n",
    "    importance_df_gb = pd.DataFrame({'Feature': feature_names, 'Importance': gb_regressor.feature_importances_}).sort_values(by='Importance', ascending=False)\n",
    "    print(importance_df_gb.head())\n",
    "else:\n",
    "    print(\"The lengths of feature_names and feature_importances_gb do not match\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {
    "id": "bbdde135"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1f98da98",
    "outputId": "9399c2a2-3b17-4655-f707-96a2f13d0954"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid_gb = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.1, 0.05, 0.01]\n",
    "}\n",
    "\n",
    "grid_search_gb = GridSearchCV(estimator=gb_regressor, param_grid=param_grid_gb, cv=3, scoring='r2', n_jobs=-1)\n",
    "grid_search_gb.fit(X_train, y_train)\n",
    "print(\"Best parameters for Gradient Boosting:\", grid_search_gb.best_params_)\n",
    "print(\"Best R² for Gradient Boosting:\", grid_search_gb.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {
    "id": "6478cde5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {
    "id": "e89afc86"
   },
   "source": [
    "### Support Vector Regression (SVR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "798d5d44",
    "outputId": "17e0abc4-0139-4e78-81ba-beada29128d5"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Initialize the SVR\n",
    "svr_regressor = SVR(kernel='rbf', C=1.0, epsilon=0.1)  # RBF kernel, adjust C and epsilon as needed\n",
    "\n",
    "# Fit the model on the training data\n",
    "svr_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_svr = svr_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_svr = mean_squared_error(y_test, y_pred_svr)\n",
    "r2_svr = r2_score(y_test, y_pred_svr)\n",
    "print(f\"SVR MSE: {mse_svr}\")\n",
    "print(f\"SVR R²: {r2_svr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {
    "id": "7d3e9b8d"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Parameter grid for SVR\n",
    "param_grid_svr = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': [0.01, 0.1, 1],\n",
    "    'epsilon': [0.01, 0.1, 0.2],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "grid_search_svr = GridSearchCV(estimator=svr_regressor, param_grid=param_grid_svr, cv=3, scoring='r2', verbose=1, n_jobs=4)\n",
    "grid_search_svr.fit(X_train, y_train)\n",
    "\n",
    "# Output the best parameters and the best score\n",
    "print(\"Best parameters for SVR:\", grid_search_svr.best_params_)\n",
    "print(\"Best R² for SVR:\", grid_search_svr.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {
    "id": "30c59374"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {
    "id": "2ae79491"
   },
   "source": [
    "### Neural Network Regression with MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8c3deee8",
    "outputId": "39a5da40-64ea-4135-eb94-a667740a9d92"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Initialize the MLPRegressor\n",
    "mlp_regressor = MLPRegressor(hidden_layer_sizes=(100,), activation='relu', solver='adam', alpha=0.0001,\n",
    "                             batch_size='auto', learning_rate='constant', learning_rate_init=0.001, max_iter=200,\n",
    "                             random_state=42, verbose=True, n_iter_no_change=10, tol=0.0001)\n",
    "\n",
    "# Fit the model on the training data\n",
    "mlp_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_mlp = mlp_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_mlp = mean_squared_error(y_test, y_pred_mlp)\n",
    "r2_mlp = r2_score(y_test, y_pred_mlp)\n",
    "print(f\"MLP MSE: {mse_mlp}\")\n",
    "print(f\"MLP R²: {r2_mlp}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {
    "id": "b709c270"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "204ea872",
    "outputId": "8016568a-fe4d-4a85-9ada-984984df944e"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Initialize the MLPRegressor with very basic settings\n",
    "mlp_regressor = MLPRegressor(max_iter=300, random_state=42, solver='lbfgs')  # Changed solver to 'lbfgs' for stability\n",
    "\n",
    "# Define an even more restricted parameter grid\n",
    "param_grid_mlp = {\n",
    "    'hidden_layer_sizes': [(50,)],  # Single layer with fewer neurons\n",
    "    'activation': ['tanh'],  # Only one activation function\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV with no parallel jobs\n",
    "grid_search_mlp = GridSearchCV(estimator=mlp_regressor, param_grid=param_grid_mlp, cv=3, scoring='r2', verbose=1, n_jobs=1)\n",
    "\n",
    "# Fit GridSearchCV\n",
    "\n",
    "grid_search_mlp.fit(X_train, y_train)\n",
    "print(\"Best parameters for MLP:\", grid_search_mlp.best_params_)\n",
    "print(\"Best R² for MLP:\", grid_search_mlp.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {
    "id": "46c2fd1c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96",
   "metadata": {
    "id": "23637b33"
   },
   "source": [
    "### Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {
    "id": "f5473b81"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "b1ec2bd1",
    "outputId": "a9f05b74-797c-4cb2-f99b-2bf0c505eed4"
   },
   "outputs": [],
   "source": [
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {
    "id": "ec2ccb56"
   },
   "outputs": [],
   "source": [
    "N_cleaned_df=yield_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {
    "id": "43d2d342"
   },
   "outputs": [],
   "source": [
    "# Convert 'Year' to a datetime type if necessary and sort the data\n",
    "N_cleaned_df['Year'] = pd.to_datetime(N_cleaned_df['Year'], format='%Y')\n",
    "N_cleaned_df.sort_values('Year', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {
    "id": "324a9fbb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 902
    },
    "id": "f50e3a0b",
    "outputId": "407bb1df-b3f2-4649-9a3a-0ad86d5ff177"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Using the cleaned and transformed data for crop yield\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.lineplot(x='Year', y='log_hg/ha_yield', data=N_cleaned_df, marker='o')\n",
    "plt.title('Trend of Log-Transformed Crop Yield Over Time')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Log-Transformed Crop Yield (log(hg/ha))')\n",
    "\n",
    "# Original data for average rainfall\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.lineplot(x='Year', y='average_rain_fall_mm_per_year', data=N_cleaned_df, marker='o', color='blue')\n",
    "plt.title('Trend of Average Rainfall Over Time')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Average Rainfall (mm)')\n",
    "\n",
    "# Original data for average temperature\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.lineplot(x='Year', y='avg_temp', data=N_cleaned_df, marker='o', color='green')\n",
    "plt.title('Trend of Average Temperature Over Time')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Average Temperature (°C)')\n",
    "\n",
    "# Using the cleaned and transformed data for pesticide tonnes\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.lineplot(x='Year', y='pesticides_tonnes', data=N_cleaned_df, marker='o', color='red')\n",
    "plt.title('Trend of Pesticides Tonnes Over Time')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Pesticides (tonnes)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103",
   "metadata": {
    "id": "65ce5a80"
   },
   "source": [
    "Rainfall and Temperature: These variables show their trends over time, which are crucial for understanding environmental impacts on crop production.\n",
    "Pesticides: The trend in pesticide usage over time can correlate with changes in agricultural practices and technological advancements in farming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "8be84943",
    "outputId": "0fd17618-8f24-4942-ebcf-25d9691d2abd"
   },
   "outputs": [],
   "source": [
    "N_cleaned_df.index.unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {
    "id": "23424329"
   },
   "outputs": [],
   "source": [
    "N_cleaned_df.set_index('Year', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d664547e",
    "outputId": "a73480c6-9504-47c2-f25c-030ee5e8eeed"
   },
   "outputs": [],
   "source": [
    "N_cleaned_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3b19b9f2",
    "outputId": "86b85333-eec8-4704-8bfa-fb0c6171464b"
   },
   "outputs": [],
   "source": [
    "# Print current frequency\n",
    "print(\"Current frequency:\", N_cleaned_df.index.freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4c86c2b4",
    "outputId": "61fd03d3-c60b-4b53-e5b9-9c0445402f0e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Check if 'Year' is the index\n",
    "if 'Year' not in N_cleaned_df.columns:\n",
    "    N_cleaned_df.reset_index(inplace=True)\n",
    "\n",
    "# Convert 'Year' to a datetime type if necessary\n",
    "if not np.issubdtype(N_cleaned_df['Year'].dtype, np.datetime64):\n",
    "    N_cleaned_df['Year'] = pd.to_datetime(N_cleaned_df['Year'], format='%Y')\n",
    "\n",
    "# Set 'Year' as the index\n",
    "N_cleaned_df.set_index('Year', inplace=True)\n",
    "\n",
    "# Ensure current frequency is checked\n",
    "print(\"Current frequency:\", N_cleaned_df.index.freq)\n",
    "\n",
    "# Aggregate data annually by taking the mean for numeric columns only\n",
    "numeric_columns = N_cleaned_df.select_dtypes(include=[np.number]).columns\n",
    "annual_data = N_cleaned_df[numeric_columns].groupby(N_cleaned_df.index.year).mean()\n",
    "\n",
    "# Creating a new DateTimeIndex with annual start frequency\n",
    "new_index = pd.date_range(start=f\"{annual_data.index.min()}-01-01\", periods=len(annual_data), freq='AS')\n",
    "\n",
    "# Assign the new index to the DataFrame\n",
    "annual_data.index = new_index\n",
    "\n",
    "# Verify the data\n",
    "print(annual_data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "e8493633",
    "outputId": "777f8941-3663-4a5f-e2c1-8f32db71bb22"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Perform seasonal decomposition\n",
    "decomposition = seasonal_decompose(annual_data['log_hg/ha_yield'], model='additive')\n",
    "\n",
    "# Plotting the decomposed components\n",
    "fig = decomposition.plot()\n",
    "fig.set_size_inches(14, 7)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(trend, label='Trend')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {
    "id": "9e6496a4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "111",
   "metadata": {
    "id": "01f98a7b"
   },
   "source": [
    "### checking for stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 622
    },
    "id": "e54310f7",
    "outputId": "8a589d44-b968-438b-a3b9-5378987ab276"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the rolling mean and rolling standard deviation\n",
    "rolling_mean = annual_data['log_hg/ha_yield'].rolling(window=12).mean()  # The window size can be adjusted\n",
    "rolling_std = annual_data['log_hg/ha_yield'].rolling(window=12).std()\n",
    "\n",
    "# Plot the original data, the rolling mean, and the rolling standard deviation\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(annual_data['log_hg/ha_yield'], color='blue', label='Original')\n",
    "plt.plot(rolling_mean, color='red', label='Rolling Mean')\n",
    "plt.plot(rolling_std, color='black', label='Rolling Std')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Rolling Mean & Standard Deviation')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {
    "id": "f97a9102"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "28739b6f",
    "outputId": "6e4ba576-a6d8-474d-f0eb-cdb42e889c35"
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Perform the Augmented Dickey-Fuller test to check for stationarity\n",
    "result = adfuller(annual_data['log_hg/ha_yield'])\n",
    "\n",
    "# Output the results\n",
    "print('ADF Statistic:', result[0])\n",
    "print('p-value:', result[1])\n",
    "print('Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "    print('\\t%s: %.3f' % (key, value))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115",
   "metadata": {
    "id": "f8aa1edc"
   },
   "source": [
    "ADF Statistic:\n",
    "−\n",
    "2.2698\n",
    "−2.2698, which is greater than any of the critical values at common significance levels (1%, 5%, and 10%). This means we fail to reject the null hypothesis of the ADF test, which states that there is a unit root present in the series.\n",
    "p-value:\n",
    "0.1819\n",
    "0.1819, which is above 0.05, suggesting that we cannot reject the null hypothesis at the 5% significance level. This indicates that the series may have a unit root, implying it is non-stationary.\n",
    "Critical Values: The test statistic is not lower than any of these, reinforcing the non-rejection of the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {
    "id": "4bc4e2c7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624
    },
    "id": "0f1c379c",
    "outputId": "b16616ae-ba5d-4dd3-ef36-f78381f26148"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import boxcox\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Apply the Box-Cox Transformation to a specific column\n",
    "data = cleaned_df['hg/ha_yield']  # Make sure this column has no zero or negative values\n",
    "data_transformed, lambda_ = boxcox(data)\n",
    "\n",
    "# Define the inverse Box-Cox transformation\n",
    "def inv_boxcox(y, lmbda):\n",
    "    if lmbda == 0:\n",
    "        return np.exp(y)\n",
    "    else:\n",
    "        return np.exp(np.log(lmbda * y + 1) / lmbda)\n",
    "\n",
    "# You can also transform the data back if needed using the lambda value obtained\n",
    "data_original = inv_boxcox(data_transformed, lambda_)\n",
    "\n",
    "# Plotting the original and transformed data for comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(data, bins=30, color='blue', alpha=0.7)\n",
    "plt.title('Original Data')\n",
    "plt.xlabel('Values')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(data_transformed, bins=30, color='green', alpha=0.7)\n",
    "plt.title('Box-Cox Transformed Data')\n",
    "plt.xlabel('Transformed Values')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Lambda used for Box-Cox Transformation:\", lambda_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2d0940f4",
    "outputId": "9599de8d-7cf0-4620-8143-b2cba500fc13"
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Perform the Augmented Dickey-Fuller test on the Box-Cox transformed data\n",
    "adf_result = adfuller(data_transformed)\n",
    "\n",
    "print('ADF Statistic:', adf_result[0])\n",
    "print('p-value:', adf_result[1])\n",
    "print('Critical Values:')\n",
    "for key, value in adf_result[4].items():\n",
    "    print('\\t%s: %.3f' % (key, value))\n",
    "\n",
    "# Evaluate the p-value and ADF Statistic against critical values\n",
    "if adf_result[1] < 0.05:\n",
    "    print(\"The series is stationary.\")\n",
    "else:\n",
    "    print(\"The series is not stationary and may need further differencing or transformations.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119",
   "metadata": {
    "id": "e5dd87de"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "120",
   "metadata": {
    "id": "90649c27"
   },
   "source": [
    "### Autocorrelation Function (ACF) & Partial Autocorrelation Function (PACF)\n",
    "\n",
    "Autocorrelation Function (ACF):\n",
    "Measures the correlation between time series observations at different lags.\n",
    "\n",
    "Partial Autocorrelation Function (PACF):\n",
    "Measures the correlation between observations at different lags while controlling for the values of the intermediate lags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5ecd1f1c",
    "outputId": "3b61e256-f0b0-47dc-94b8-0176009422ac"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from matplotlib.pylab import rcParams\n",
    "\n",
    "# Set the plot size and style for better visibility\n",
    "rcParams['figure.figsize'] = (14, 10)  # Increase plot size\n",
    "\n",
    "\n",
    "# Plotting ACF with adjusted alpha for a clearer view of the confidence interval\n",
    "plt.figure()\n",
    "plot_acf(data_transformed, lags=10, alpha=0.05)\n",
    "plt.title('Autocorrelation Function')\n",
    "\n",
    "# Plotting PACF with adjusted alpha\n",
    "plt.figure()\n",
    "plot_pacf(data_transformed, lags=10, alpha=0.05, method='ywm')\n",
    "plt.title('Partial Autocorrelation Function')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {
    "id": "95374aff"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "123",
   "metadata": {
    "id": "6952865e"
   },
   "source": [
    "### Modelling the Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124",
   "metadata": {
    "id": "b31b4c84"
   },
   "source": [
    "### AutoRegressive (AR) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "69fee673",
    "outputId": "0598c53f-822f-42ec-cb32-7ddecb419604"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# ARIMA(p,d,q) where p=2, d=0, q=0\n",
    "model = ARIMA(data_transformed, order=(2,0,0))\n",
    "results = model.fit()\n",
    "\n",
    "# Print the summary of the model\n",
    "print(results.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126",
   "metadata": {
    "id": "faecdf52"
   },
   "source": [
    "\n",
    "No. Observations: 25,874 observations were used in this model, indicating a large dataset.\n",
    "\n",
    "Model: ARIMA(2, 0, 0) suggests that this is an AutoRegressive (AR) model with 2 lags (p=2), no differencing (d=0), and no moving average components (q=0).\n",
    "\n",
    "Log Likelihood: -30,656.324 - This value indicates the log likelihood of the model, where higher values are better. In your case, the value is negative and quite large, suggesting potential overfitting or a poor model fit due to the complexity or noise in the data.\n",
    "\n",
    "AIC: 61,320.649 - The Akaike Information Criterion helps compare different models. Lower AIC values are better. Given the size of your AIC, it might suggest that the model could be improved either by simplifying or by better parameter tuning.\n",
    "\n",
    "BIC: 61,353.293 - The Bayesian Information Criterion, like AIC, helps in model comparison. It usually penalizes free parameters more strongly than AIC. The high value here again suggests room for model improvement.\n",
    "\n",
    "Covariance Type: 'opg' suggests that the 'outer product of gradients' method was used for covariance matrix calculation.\n",
    "\n",
    "Coefficients:\n",
    "const: The constant term is 9.9497, with a very tight confidence interval, indicating a strong and significant baseline level of the transformed yield data.\n",
    "ar.L1: The coefficient for the first lag (0.4411) is significant (P>|z| near 0), indicating a strong positive correlation with the previous value in the series.\n",
    "ar.L2: The coefficient for the second lag (0.1642) is also significant and positive, showing that the second past value also influences the current value but to a lesser extent than the first lag.\n",
    "sigma2: The variance of the residuals is 0.6261, suggesting the average squared deviations from the predicted values are relatively low, indicating a good fit of the model to the data.\n",
    "\n",
    "Diagnostic Tests:\n",
    "Ljung-Box (L1) (Q): A p-value of 0.01 suggests that there may still be some autocorrelation at lag 1 that the model does not account for.\n",
    "Jarque-Bera (JB): A p-value close to 0 indicates that the residuals of the model do not follow a normal distribution.\n",
    "Heteroskedasticity (H): The test suggests that the variance of the residuals is not constant. The p-value close to 0 indicates that the assumption of homoscedasticity (constant variance) is violated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127",
   "metadata": {
    "id": "bd1b3103"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "128",
   "metadata": {
    "id": "3e6c604f"
   },
   "source": [
    "### MA Model (Moving Average Model)\n",
    "\n",
    "An MA model is another type of time series model that uses dependency between an observation and a residual error from a moving average model applied to lagged observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a6d3d09d",
    "outputId": "f5498dce-2fd7-48b9-ad1f-f0ed38ac4fe5"
   },
   "outputs": [],
   "source": [
    "# Set the order (q)\n",
    "q = 1\n",
    "\n",
    "# Create and fit the MA model\n",
    "ma_model = ARIMA(data_transformed, order=(2,0,q))\n",
    "ma_results = ma_model.fit()\n",
    "\n",
    "# Print the summary of the MA model\n",
    "print(ma_results.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130",
   "metadata": {
    "id": "bbab48d4"
   },
   "source": [
    "Log Likelihood: -226.336. This value indicates the likelihood of the model given the data, where higher values (closer to zero) generally indicate a better fit.\n",
    "\n",
    "AIC (Akaike Information Criterion): 458.672. This metric helps to compare models; lower values suggest a better model fit considering the number of parameters used.\n",
    "\n",
    "BIC (Bayesian Information Criterion): 462.078. This is another criterion for model comparison that penalizes free parameters more strongly than AIC.\n",
    "\n",
    "const: The estimated constant term is approximately 76,870, suggesting the baseline level of hg/ha_yield in the absence of other influences.\n",
    "\n",
    "ma.L1: The coefficient for the first lag of the moving average term is 0.9993. This indicates a near-complete carryover of last year's forecast error into this year's prediction, which suggests that the previous year's error almost entirely influences the current year's forecast.\n",
    "\n",
    "Ljung-Box Test (Q): With a statistic of 13.45 and a p-value near zero, this suggests that there are still autocorrelations in the residuals not captured by the model.\n",
    "\n",
    "Jarque-Bera (JB): This test checks the normality of residuals. The JB statistic of 1.53 and a p-value of 0.47 do not provide strong evidence against the normality assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131",
   "metadata": {
    "id": "92064352"
   },
   "source": [
    "Interpretation:\n",
    "\n",
    "While the MA model captures some of the error dynamics from one year to the next, the significant Ljung-Box statistic suggests that additional lags or perhaps a combination of autoregressive and moving average terms might be necessary to adequately model the data. The near-unit root in the MA term suggests that the series might benefit from additional differencing or a model that captures longer-term trends or cyclical behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132",
   "metadata": {
    "id": "f10e5815"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "133",
   "metadata": {
    "id": "15c18798"
   },
   "source": [
    "### AutoRegressive Moving Average (ARMA) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7f716ec3",
    "outputId": "9fe3cfc5-3a8b-4cc7-8518-fd224feb4606"
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Setting the AR and MA components\n",
    "p = 2\n",
    "q = 2\n",
    "\n",
    "# Creating and fitting the ARMA model (ARIMA with d=0)\n",
    "arma_model = ARIMA(data_transformed, order=(p, 0, q))\n",
    "arma_results = arma_model.fit()\n",
    "\n",
    "# Print the summary of the model\n",
    "print(arma_results.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135",
   "metadata": {
    "id": "ddd4562e"
   },
   "source": [
    "Model: ARIMA(2, 0, 2) indicates two AR terms and two MA terms without differencing.\n",
    "Sample: 25,874 observations, indicating a substantial amount of data for the time series analysis.\n",
    "Covariance Type: 'opg' indicates that the outer product of gradients method was used for estimation.\n",
    "\n",
    "\n",
    "Log Likelihood: -30,241.695, which provides a measure of the model's goodness of fit; higher values (less negative) are better.\n",
    "AIC (Akaike Information Criterion): 60,495.391 helps compare models; lower values suggest a better model relative to others by balancing goodness of fit and complexity.\n",
    "BIC (Bayesian Information Criterion): 60,544.357, similar to AIC, but with a higher penalty for models with more parameters.\n",
    "\n",
    "\n",
    "const: The model's constant is approximately 9.9496, which could represent the series' mean level after considering the AR and MA components.\n",
    "ar.L1 and ar.L2: Coefficients for the first and second lags of the AR component. ar.L1 is -0.2064, suggesting a negative influence from the first lag on the current value. ar.L2 is 0.7733, indicating a strong positive effect from the second lag.\n",
    "ma.L1 and ma.L2: Coefficients for the MA terms. ma.L1 is 0.6154, showing a positive adjustment based on the first lag's error, and ma.L2 is -0.3396, indicating a negative adjustment based on the second lag's error.\n",
    "\n",
    "\n",
    "Ljung-Box Test: The Q statistic at lag 1 is 23.81 with a p-value of 0.00, suggesting significant autocorrelation in residuals at lag 1.\n",
    "Jarque-Bera (JB) Test: A JB statistic of 89.34 with a p-value of 0.00 indicates that residuals do not follow a normal distribution.\n",
    "Heteroskedasticity (H): A heteroskedasticity statistic of 1.51 with a p-value of 0.00 suggests variance inconsistency in the residuals over time.\n",
    "Skew and Kurtosis: The skewness of 0.05 and kurtosis of 3.27 suggest slight asymmetry and a peak near the normal distribution.\n",
    "\n",
    "\n",
    "Summary:\n",
    "The ARIMA(2, 0, 2) model seems to fit the data adequately, with significant parameters indicating a complex interplay of autoregressive and moving average components. However, the diagnostics highlight potential issues with autocorrelation and non-normal distribution of residuals, suggesting further refinement or additional diagnostic checks may be necessary to fully validate the model's assumptions and effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ddd279c1",
    "outputId": "8aac49ff-9c24-4f09-ac35-8dcefcb41984"
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Setting the AR and MA components\n",
    "p = 3\n",
    "q = 3\n",
    "\n",
    "# Creating and fitting the ARMA model (ARIMA with d=0)\n",
    "arma_model = ARIMA(data_transformed, order=(p, 0, q))\n",
    "arma_results = arma_model.fit()\n",
    "\n",
    "# Print the summary of the model\n",
    "print(arma_results.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137",
   "metadata": {
    "id": "e4130511"
   },
   "source": [
    "\n",
    "Model: ARIMA(3, 0, 3) indicating three AR terms and three MA terms.\n",
    "Sample Size: 25,874 observations, providing a substantial dataset for robust statistical analysis.\n",
    "Covariance Type: 'opg' indicating the use of the outer product of gradient estimator for covariance computation.\n",
    "Statistical Performance:\n",
    "Log Likelihood: -30,013.905, indicating the model's likelihood given the data. Higher (less negative) values indicate better model fit.\n",
    "AIC (Akaike Information Criterion): 60,043.809, which helps to compare models; lower values generally indicate a better fit considering the complexity and number of parameters.\n",
    "BIC (Bayesian Information Criterion): 60,109.097, similar to AIC but with a stronger penalty for more complex models, useful for model selection.\n",
    "Parameter Estimates:\n",
    "Constant: The constant term (const) approximates 9.9477, suggesting the baseline level around which the series fluctuates.\n",
    "AR and MA Coefficients:\n",
    "ar.L1: Coefficient for the first AR term is 0.5702, indicating a moderate positive influence from the first lag.\n",
    "ar.L2: Coefficient for the second AR term is 0.9741, showing a strong positive influence from the second lag.\n",
    "ar.L3: Coefficient for the third AR term is -0.5580, suggesting a significant negative impact from the third lag.\n",
    "ma.L1: Coefficient for the first MA term is -0.1480, a slight negative influence reflecting the error of the first lag.\n",
    "ma.L2: Coefficient for the second MA term is -0.9139, indicating a strong negative adjustment based on the second lag's error.\n",
    "ma.L3: Coefficient for the third MA term is 0.1497, showing a modest positive adjustment based on the third lag's error.\n",
    "Diagnostics:\n",
    "Ljung-Box Test (Q): A p-value of 0.24 at lag 1 indicates there is no significant autocorrelation in the residuals, which is good as it suggests the model adequately captures the correlation in the data.\n",
    "Jarque-Bera (JB) Test: A statistic of 79.12 with a p-value near zero indicates that residuals do not follow a normal distribution, suggesting potential issues with the model assumptions.\n",
    "Heteroskedasticity (H): A test value of 1.52 with a p-value near zero suggests the presence of heteroskedasticity, meaning the variance of residuals is not constant across the series.\n",
    "Skew and Kurtosis: Skew of 0.11 and kurtosis of 3.16 suggest slight asymmetry and a distribution peak near the normal.\n",
    "Conclusion:\n",
    "The ARIMA(3, 0, 3) model demonstrates significant predictive power given its statistical measures and coefficients. However, issues such as non-normal residuals and potential heteroskedasticity should be addressed, possibly through further model refinement or additional diagnostic checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138",
   "metadata": {
    "id": "b00cdc54"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139",
   "metadata": {
    "id": "34b7b014"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140",
   "metadata": {
    "id": "3fa42663"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141",
   "metadata": {
    "id": "166c06ec"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142",
   "metadata": {
    "id": "5f52f4f4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143",
   "metadata": {
    "id": "af535988"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144",
   "metadata": {
    "id": "91bd7480"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "145",
   "metadata": {
    "id": "0d05adce"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146",
   "metadata": {
    "id": "40c0eda0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "147",
   "metadata": {
    "id": "eb54b748"
   },
   "source": [
    "Additional Steps:\n",
    "Collect More Data: If possible, enhance your dataset with more variables that could affect crop yields, such as soil quality data, crop variety information, and more detailed local climate data.\n",
    "\n",
    "Stakeholder Engagement: Work with local farmers, agricultural experts, and climatologists to refine your models and recommendations to ensure they are practical and grounded in local realities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148",
   "metadata": {
    "id": "c4tyQQ31Zyfg"
   },
   "source": [
    "**Now we deploy the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149",
   "metadata": {
    "id": "4irh1pb3ZyBd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0d4U5Z88Od_y",
    "outputId": "ba30cd1c-47c1-4d5a-b46b-1451f8a4dc72"
   },
   "outputs": [],
   "source": [
    "!pip install ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pc8tdvpNX92T",
    "outputId": "8b536794-d771-48e2-9777-1b4d800c84b9"
   },
   "outputs": [],
   "source": [
    "print(preprocessor.named_transformers_['cat'].categories_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209,
     "referenced_widgets": [
      "32ae49ceddb8449db2710f19c523a8d0",
      "7b7fe2ccec864e8b97b7ce9a8430852b",
      "6432a8719b9b4d038f8ae8e629e3e8ca",
      "ebf03fccb7fd4281a05c03e413523343",
      "f07cb41e26b245a7bf1b86a25b5a02a0",
      "b18918b0611e49dfb1a4edbceec49059",
      "993fcde1581e45a2982a64108596fdcb",
      "f5a3f34d39474515842567f607254e07",
      "202bb33660a14673b50414b7eea3f687",
      "e1fe7016d1044f2684ab090b8ddb7e37",
      "4b4deb02e44d4aee8f4082be514b5711",
      "a32044b7ecb54060a50910fd4ad6dcb2",
      "152cbbbdb5dd4c8a8465f38326aa4ad7",
      "3ce17c142944464da131a83faab2cff6",
      "a39ac9882b814d4db48410619e80517a",
      "69b2d66598654c0589d9466123ba0c74",
      "485191ba3bcd4210aa0f157d362c0f34",
      "4779cdd308a940299b26a93e6388bd1d",
      "62513f2a3e4c4663825b333bce3130f6",
      "a7e2dafcf4944d438c95722361fc93ba",
      "f27db9c5d359444693a05e6b4e0ce3d7",
      "38276af3ef5a46d78126f4a489b81cea"
     ]
    },
    "id": "dFSa4ZBDY0YQ",
    "outputId": "14f14a30-52ce-4560-fe32-b386b0e50af7"
   },
   "outputs": [],
   "source": [
    "def on_button_click(b):\n",
    "    input_data = {\n",
    "        'avg_temp': [inputs['avg_temp'].value],\n",
    "        'average_rain_fall_mm_per_year': [inputs['average_rain_fall_mm_per_year'].value],\n",
    "        'pesticides_tonnes': [inputs['pesticides_tonnes'].value],\n",
    "        'Area': ['Argentina'],  # Use a valid area from your categories\n",
    "        'Item': ['Maize']  # Use a valid item from your categories\n",
    "    }\n",
    "\n",
    "    input_df = pd.DataFrame(input_data)\n",
    "\n",
    "    # Preprocess the input data\n",
    "    try:\n",
    "        preprocessed_data = preprocessor.transform(input_df)\n",
    "\n",
    "        # Make prediction\n",
    "        prediction = linear_model.predict(preprocessed_data)\n",
    "\n",
    "        # Display the prediction\n",
    "        output.clear_output()\n",
    "        with output:\n",
    "            display(f\"Predicted Yield: {prediction[0]:.2f} log(hg/ha)\")\n",
    "    except ValueError as e:\n",
    "        output.clear_output()\n",
    "        with output:\n",
    "            display(f\"Error: {e}\")\n",
    "\n",
    "button.on_click(on_button_click)\n",
    "\n",
    "# Display all widgets\n",
    "display(widgets.VBox([*inputs.values(), button, output]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 227,
     "referenced_widgets": [
      "f4740fb71e2046099c26b5cc0a2a386c",
      "99b144cf3a90497daba396a7f44e7f2d",
      "b8ef9459a49c49f4b6d358bba7266b4e",
      "42f7e26b33464856866d12d2a9c7c94f",
      "943fb870ac70469189954fa431a29744",
      "6ec831d6ab544d9486dde45801c3fa2f",
      "896b9b7a3c9146bbbea950983ed9eb75",
      "ee4329c6aa3248bf85a2dd162ce949d6",
      "85a3d2adf307494bbf3aced8685d7c5e",
      "4a4374c3ded945aaa8cb57b4b9020687",
      "f1b452b689934977baf8e0d77d730633",
      "2a9f16a5d01043c195c4d9721cafa4ec",
      "20703073595244b49dd7aba693603ed8",
      "39404e437eed46669c5eafa36b0f6bff",
      "a716895d416b44f1bb49a9d3658a02a7",
      "e05bb99fec564ad79bc9623aba856421",
      "ecaa7fefaa794214bd6c996d892e63e8",
      "6fb7dc8684d0458fad7f6811162e64f6",
      "98b0a7aa91624af3823854723ec0ba2a",
      "5a766b0b4bfa4637aaa87e73c5e7cb8a",
      "8e1d5d55ad0b47a5aab6dc6d74fd46a8",
      "db7d160655944ebbae238a0c5f50cde1"
     ]
    },
    "id": "shT8Kq3_OuFx",
    "outputId": "988a53ce-1411-470c-e116-eeb47fe45d87"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Define input widgets\n",
    "inputs = {\n",
    "    'avg_temp': widgets.FloatText(description='avg_temp', value=25),\n",
    "    'average_rain_fall_mm_per_year': widgets.FloatText(description='average_rain_fall_mm_per_year', value=800),\n",
    "    'pesticides_tonnes': widgets.FloatText(description='pesticides_tonnes', value=100),\n",
    "    'Area': widgets.Dropdown(description='Area', options=['Argentina', 'Australia', 'Brazil', 'Canada', 'China']),\n",
    "    'Item': widgets.Dropdown(description='Item', options=['Maize', 'Potatoes', 'Wheat', 'Soybeans', 'Rice, paddy'])\n",
    "}\n",
    "\n",
    "# Button to trigger the prediction\n",
    "button = widgets.Button(description=\"Predict\")\n",
    "\n",
    "# Output widget to display the prediction\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_button_click(b):\n",
    "    input_data = {\n",
    "        'avg_temp': [inputs['avg_temp'].value],\n",
    "        'average_rain_fall_mm_per_year': [inputs['average_rain_fall_mm_per_year'].value],\n",
    "        'pesticides_tonnes': [inputs['pesticides_tonnes'].value],\n",
    "        'Area': [inputs['Area'].value],\n",
    "        'Item': [inputs['Item'].value]\n",
    "    }\n",
    "\n",
    "    input_df = pd.DataFrame(input_data)\n",
    "\n",
    "    # Preprocess the input data\n",
    "    try:\n",
    "        preprocessed_data = preprocessor.transform(input_df)\n",
    "\n",
    "        # Make prediction\n",
    "        prediction = linear_model.predict(preprocessed_data)\n",
    "\n",
    "        # Display the prediction\n",
    "        output.clear_output()\n",
    "        with output:\n",
    "            display(f\"Predicted Yield: {prediction[0]:.2f} log(hg/ha)\")\n",
    "    except ValueError as e:\n",
    "        output.clear_output()\n",
    "        with output:\n",
    "            display(f\"Error: {e}\")\n",
    "\n",
    "button.on_click(on_button_click)\n",
    "\n",
    "# Display all widgets\n",
    "display(widgets.VBox([*inputs.values(), button, output]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
